{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "image-classification ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import cifar10\nimport matplotlib.pyplot as plt\n\n# 1. Load CIFAR-10 Dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# 2. Define a Simple CNN Model\ndef create_model():\n    model = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10)\n    ])\n    return model\n\n# 3. Compile and Train Function for Experimentation with Different Optimizers\ndef compile_and_train(model, optimizer):\n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n    \n    # Train the model\n    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), batch_size=64)\n    return history\n\n# 4. Experimenting with Different Optimizers\noptimizers = ['sgd', 'adam', 'rmsprop', 'adagrad']\nhistories = {}\n\nfor opt in optimizers:\n    print(f\"Training with {opt} optimizer:\")\n    model = create_model()\n    history = compile_and_train(model, opt)\n    histories[opt] = history\n\n# 5. Plotting the Results\nplt.figure(figsize=(12, 8))\n\nfor opt in optimizers:\n    history = histories[opt]\n    \n    # Plot Accuracy\n    plt.subplot(2, 2, 1)\n    plt.plot(history.history['accuracy'], label=f'{opt} train accuracy')\n    plt.plot(history.history['val_accuracy'], label=f'{opt} val accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Plot Loss\n    plt.subplot(2, 2, 2)\n    plt.plot(history.history['loss'], label=f'{opt} train loss')\n    plt.plot(history.history['val_loss'], label=f'{opt} val loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Define the activation function (ReLU and Sigmoid)\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Define the forward propagation function\ndef forward_propagation(X, W1, b1, W2, b2):\n    # Layer 1: Hidden Layer\n    Z1 = np.dot(X, W1) + b1  # Linear transformation\n    A1 = relu(Z1)             # Activation function (ReLU)\n\n    # Layer 2: Output Layer\n    Z2 = np.dot(A1, W2) + b2  # Linear transformation\n    A2 = sigmoid(Z2)          # Activation function (Sigmoid)\n\n    return A1, A2  # Return activations from hidden and output layers\n\n# Example of input data (4 samples, 3 features)\nX = np.array([[0.1, 0.2, 0.3],\n              [0.4, 0.5, 0.6],\n              [0.7, 0.8, 0.9],\n              [1.0, 1.1, 1.2]])\n\n# Define the weights and biases for the network\nnp.random.seed(42)  # For reproducibility\nW1 = np.random.randn(3, 4)  # 3 input features, 4 neurons in the hidden layer\nb1 = np.zeros((1, 4))       # Bias for the hidden layer\nW2 = np.random.randn(4, 1)  # 4 neurons in the hidden layer, 1 output neuron\nb2 = np.zeros((1, 1))       # Bias for the output layer\n\n# Perform forward propagation\nA1, A2 = forward_propagation(X, W1, b1, W2, b2)\n\nprint(\"Hidden Layer Activations (A1):\")\nprint(A1)\nprint(\"\\nOutput Layer Activations (A2):\")\nprint(A2)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}